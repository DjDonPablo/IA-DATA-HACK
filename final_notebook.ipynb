{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AI vs IA\n",
    "---\n",
    "\n",
    "#### Groupe 12\n",
    "\n",
    "Ce notebook contient notre travail pour le sujet **AI vs IA** du hackaton **IA Data Hack**<br><br>\n",
    "La structure du notebook est la suivante:\n",
    "- Compréhension du sujet\n",
    "- Analyse et transformation des données\n",
    "- Benchmark de différents modèles de machine learning\n",
    "- Optimisation du meilleur modèle de machine learning\n",
    "- Modèles de deep learning\n",
    "- Conclusion\n",
    "\n",
    "Nous utilisons la librairie ```scikit-learn``` pour le benchmark, qui s'exécute seulement sur CPU, donc ça prendra beaucoup de temps d'entraîner les différents modèles.<br>\n",
    "Vous pouvez directement trouver dans la conclusion une cellule avec notre meilleur modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compréhension du sujet\n",
    "---\n",
    "\n",
    "Le sujet de ce hackaton est, comme dit précedemment, **AI vs IA**. A partir d'un texte, nous devons identifier si ce texte a été écrit par un **humain** ou par une **IA**.<br>\n",
    "Ainsi, on considère ce problème comme un problème de **classification**, plus précisément un problème de **classification binaire**. La classification binaire est un problème commun en intelligence artificielle, qui demande des modèles de ***machine learning*** ou de ***deep learning***. Nous allons utiliser ces 2 types de modèles et les comparer dans notre benchmark pour sélectionner le plus adapté.<br>\n",
    "Pour un problème de classification binaire, nous utilisons la fonction de perte ```binary cross entropy```, qui est la plus appropriée, et qui se calcule de la façon suivante:\n",
    "\n",
    "$$\n",
    "BCE = -y \\cdot \\log(p) - (1 - y) \\cdot \\log(1 - p)\n",
    "$$\n",
    "\n",
    "Où:\n",
    "- *y* est le vrai label (0 - IA ou 1 - Humain)\n",
    "- *p* est la probabilité prédite (entre 0 et 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse et transformation des données\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score,recall_score, precision_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from transformers import AutoTokenizer, TFBertModel \n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupération du dataset sur AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un object filesystem s3\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data.csv\"\n",
    "\n",
    "# Téléchargement des données dans le service\n",
    "fs.download(\"civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataframe à partir du fichier csv\n",
    "df = pd.read_csv(file_path).drop(['src'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Etude de la longueur des textes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataframe pour étudier la longueur des textes du dataframe\n",
    "text_df = pd.DataFrame(df[\"label\"])\n",
    "text_df['text_length'] = df[\"text\"].apply(len)\n",
    "print(\"Longueur moyenne des textes: \", int(text_df['text_length'].mean()))\n",
    "print(\"Longueur minimum des textes: \", text_df['text_length'].min())\n",
    "print(\"Longueur maximum des textes: \", text_df['text_length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, count = np.unique(text_df[text_df[\"label\"] == 1][\"text_length\"].values, return_counts=True)\n",
    "plt.bar(values, count)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Longueur du texte\")\n",
    "plt.ylabel(\"Nombre d'occurence\")\n",
    "plt.title(\"Répartition du nombre d'occurence de différentes longueurs de texte chez les humains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, count = np.unique(text_df[text_df[\"label\"] == 0][\"text_length\"].values, return_counts=True)\n",
    "plt.bar(values, count)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Longueur du texte\")\n",
    "plt.ylabel(\"Nombre d'occurence\")\n",
    "plt.title(\"Répartition du nombre d'occurence de différentes longueurs de texte chez les IAs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la longueur des textes écrits par les humains est plus étendue que celle des textes générées par des IAs, qui semble être centrée autour de 200.\n",
    "\n",
    "### Occurences de mots ayant des fautes d'ortographe\n",
    "\n",
    "La liste des mots bannis a été établi selon les fautes les plus récurrentes dans la langue anglaise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklisted_words = {\"didnt\", \"dont\", \"im\", \"youre\", \"ive\", \"doesnt\", \"wasnt\", \"couldnt\", \"thats\", \"isnt\", \"theres\", \"theyre\", \"wouldnt\", \"arent\", \"youll\", \"Dont\", \"youve\", \"havent\", \"hadnt\", \"werent\"}\n",
    "\n",
    "# Dictionnaire pour compter les occurrences des mots de la liste noire dans les textes pour chaque label\n",
    "blacklisted_counts_label_0 = {word: 0 for word in blacklisted_words}\n",
    "blacklisted_counts_label_1 = {word: 0 for word in blacklisted_words}\n",
    "\n",
    "# Parcourir chaque texte pour compter les occurrences exactes des mots de la liste noire\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    words_in_text = text.split()\n",
    "    for word in blacklisted_words:\n",
    "        for word_in_text in words_in_text:\n",
    "            if word_in_text.lower() == word.lower():\n",
    "                if row['label'] == 0:\n",
    "                    blacklisted_counts_label_0[word.lower()] += 1\n",
    "                else:\n",
    "                    blacklisted_counts_label_1[word.lower()] += 1\n",
    "\n",
    "# Filtrer les mots avec au moins une occurrence pour chaque label\n",
    "filtered_counts_label_0 = {word: count for word, count in blacklisted_counts_label_0.items() if count > 0}\n",
    "filtered_counts_label_1 = {word: count for word, count in blacklisted_counts_label_1.items() if count > 0}\n",
    "\n",
    "# Trier dictionnaire par valeur pour obtenir les mots avec le plus d'occurrences pour chaque label\n",
    "sorted_counts_label_0 = sorted(filtered_counts_label_0.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_counts_label_1 = sorted(filtered_counts_label_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extraction data pour graphique pour label 0\n",
    "words_label_0 = [word for word, count in sorted_counts_label_0]\n",
    "counts_label_0 = [count for word, count in sorted_counts_label_0]\n",
    "\n",
    "# Extraction data pour graphique pour label 1\n",
    "words_label_1 = [word for word, count in sorted_counts_label_1]\n",
    "counts_label_1 = [count for word, count in sorted_counts_label_1]\n",
    "\n",
    "# Tunning graphique pour label 0\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(words_label_0, counts_label_0, color='skyblue')\n",
    "plt.xlabel('Nombre d\\'occurrences')\n",
    "plt.ylabel('Mots')\n",
    "plt.title('Mots avec le plus de faute d\\'orthographe compris dans la liste noire \\n IA ')\n",
    "plt.gca().invert_yaxis()  \n",
    "\n",
    "# Tunning graphique pour label 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(words_label_1, counts_label_1, color='salmon')\n",
    "plt.xlabel('Nombre d\\'occurrences')\n",
    "plt.ylabel('Mots')\n",
    "plt.title('Mots avec le plus de faute d\\'orthographe compris dans la liste noire \\n Humain')\n",
    "plt.gca().invert_yaxis()  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuage de mots sur les réponses humaines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "human = df[df['label'] == 1]\n",
    "action_text = human['text'].values\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color='white', height=1000, width=2000).generate(str(action_text))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuage de mots sur les réponses d'IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "human = df[df['label'] == 0]\n",
    "action_text = human['text'].values\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color='white', height=1000, width=2000).generate(str(action_text))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que les mots les plus utilisés par les humains et les IAs sont très différents, avec pour les humains des mots comme \"data\", \"us\" ou \"people\", et pour les IAs des mots comme \"application\", \"space\" ou \"layer\".\n",
    "\n",
    "### Proportion des textes entre humains et IAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts(\"label\", normalize=True).plot(kind=\"bar\", title= \"Pourcentage des textes créés par les humains et les IAs\").set_xticklabels(('humain', 'IA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une répartition uniforme entre les 2 différents labels, ce qui facilitera l'apprentissage des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark de différents modèles de machine learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division du dataset en set de testing et de training, avec 80% pour le training et 20% pour le testing (qui nous sert aussi de set de validation par manque de donnée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement de 5 modèles de machine learning avec un grid search\n",
    "\n",
    "Le choix des modèles que nous avons choisi pour le grid search résulte de nos recherches, ces modèles semblent être les plus appropriés pour de la classification binaire de texte, mais on aurait bien aimé en rajouter d'autres qui semblaient aussi prometteur.\n",
    "\n",
    "La pipeline utilisée pour chaque modèle est plutôt simple, elle consiste d'un **CountVectorizer** qui va nous permettre de transcrire nos chaînes de caractères en vecteurs, et ensuite du modèle que l'on souhaite tester. **CountVectorizer** a été choisi après plusieurs tests avec différentes méthodes d'embedding, notamment **Word2Vec**, **FastText**, **TfIdf**, mais un vecteur avec le nombre d’occurrences de chaque mot du vocabulaire semble être le plus adapté. Nous avons choisi le tokenizer ***punkt*** de ```nltk```.<br><br>\n",
    "\n",
    "**PS: La réalisation de ce benchmark est très longue, nous vous conseillons de nous faire confiance pour notre choix du meilleur modèle de machine learning résultant de son exécution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {'model__n_estimators': [50, 100, 200]}\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {'model__n_estimators': [50, 100, 200], 'model__learning_rate': [0.01, 0.1, 1.0]}\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(),\n",
    "        'params': {'model__C': [0.1, 1, 10], 'model__gamma': ['scale', 'auto']}\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {'model__C': [0.1, 1, 10]}\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {'model__alpha': [0.01, 0.1, 1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_search_objects = {}\n",
    "results = {}\n",
    "\n",
    "for name, m in models.items():\n",
    "    pipe = Pipeline([\n",
    "        ('count_vect', CountVectorizer(tokenizer=word_tokenize, ngram_range=(2, 2))),\n",
    "        ('model', m['model'])\n",
    "    ])\n",
    "    grid_search = GridSearchCV(pipe, m['params'], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    grid_search_objects[name] = grid_search\n",
    "    results[name] = {\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f\"{name}: Meilleure précision = {res['best_score']}\")\n",
    "    print(f\"Meilleurs paramètres : {res['best_params']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation avec les données de tests des meilleurs modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, grid_search in grid_search_objects.items():\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    prediction_time = end_time - start_time\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"{name} Test Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Prediction Time: {prediction_time} seconds\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation du meilleur modèle de machine learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le benchmark que l'on vient d'effectuer, on se rend vite compte que le modèle de machine learning le plus performant est la **regression logistique**, c'est pourquoi on va continuer à améliorer ce modèle avec une plus grande *grid search* et une augmentation de la taille des ngrams du **CountVectorizer**. On va aussi utiliser une validation croisée pour avoir de meilleures métriques sur les performances de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'logisticregression__max_iter': [100, 500, 1000], 'logisticregression__tol': [1e-4, 1e-5]}\n",
    "model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range=(3, 3)), LogisticRegression())\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Meilleurs paramètres trouvés via la recherche sur la grille : {grid_search.best_params_}\")\n",
    "print(f\"Meilleure précision lors de la validation croisée : {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inférence sur les données de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log_reg = grid_search.best_estimator_\n",
    "\n",
    "start = time.time()\n",
    "y_pred = best_log_reg.predict(X_test)\n",
    "print(f\"Temps d'inférence: {time.time() - start}\")\n",
    "\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred, digits=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de confusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=final_model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles de deep learning\n",
    "---\n",
    "Conversion de la donnée en dataset **tensorflow** pour l'entrainement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).shuffle(5000).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement d'une couche de vectorisation et définition du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.TextVectorization()\n",
    "encoder.adapt(train_dataset.map(lambda text, _: text))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation, entraînement et test du modèle RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_delta=0.1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.AdamW(1e-4),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN avec BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation d'un modèle **BERT** pré-entraîné et création d'un dataset spécial avec le tokenizer lié au modèle de **BERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "max_len = 120\n",
    "X_train = tokenizer(\n",
    "    text=df_train['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=df_test['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition du modèle **BERT** et **RNN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnBert(tf.keras.Layer):\n",
    "    def call(self, ids, mask):\n",
    "        return bert(ids, attention_mask=mask)[0]\n",
    "    \n",
    "x = OwnBert()(input_ids, input_mask)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(embeddings)\n",
    "x = tf.keras.layers.Dense(64, activation='gelu')(out)\n",
    "x = tf.keras.layers.Dropout(0.5)(out)\n",
    "x = tf.keras.layers.Dense(32, activation='gelu')(out)\n",
    "x = tf.keras.layers.Dropout(0.5)(out)\n",
    "x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=x)\n",
    "model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation, entraînement et test du modèle **BERT** et **RNN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.AdamW(1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x = {'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
    "    y = df_train['label'],\n",
    "    validation_data = ({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']}, df_test['label']),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x = {'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']}, y = y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range=(3, 3)), LogisticRegression(max_iter=500))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred, digits=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre meilleur modèle est donc la régression logistique couplée au CountVectorizer de **scikit**, qui utilise le tokenizer **punkt** de ```nltk```.<br>\n",
    "CountVectorizer fait ressortir les features importantes que nous avons observé sur ce dataset lors de l'analyse des données, par les différences entre le texte créé par un humain et celui généré par une IA, notamment sur les critères de longueur de texte, la diversité des mots utilisés, le nombre de faute d'orthographes etc...<br>\n",
    "Avec les hyper-paramètres que nous avons trouvé pour la régression logistique, nous obtenons une précision d'environ **95%**.\n",
    "<br><br>\n",
    "La sous-performance (qui reste quand même acceptable) des modèles de **deep learning** n'est pas étonnante, le dataset ne permettait tout simplement pas d'obtenir un modèle très précis, d'abord à cause de sa taille réduite, mais aussi par manque de diversité dans les données. Les modèles de **deep learning** performant nécessite souvent une donnée par paramètre, ce qui n'est pas du tout notre cas, et qui empêche donc la convergence de notre modèle.\n",
    "<br><br><br>\n",
    "Le temps d'inférence (inference_time dans le tableau) est calculé pour l'inférence de **20%** du dataset, soit environ **10000** lignes.\n",
    "\n",
    "|         x          | Random forest | Gradient boost | SVM     | Naive Bayes | Logistic Reg | RNN   | BERT RNN |\n",
    "| ------------------ | ------------- | -------------- | ------- | ----------- | ------------ | ----- | -------- |\n",
    "| precision_test     | 0.86          | 0.77           | 0.79    | 0.73        | ***0.95***   | 0.92  | 0.94     |\n",
    "| temps_inference    | 8s            | 2.7s           | 195s    | ***2s***    | 3s           | 8s    | 8s       |\n",
    "| conso_entraînement | basse         | basse          | moyenne | basse       | moyenne      | haute | haute    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
